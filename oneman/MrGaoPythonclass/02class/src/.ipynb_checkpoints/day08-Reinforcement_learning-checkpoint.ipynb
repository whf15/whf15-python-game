{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是强化训练，强化训练的环境以及实现\n",
    "强化学习的目标是需要学习一种策略，使得对于每一个状态，决策AI的动作\n",
    "\n",
    "就像训练小狗一样，开始动作是无序的，做对了奖励吃的，再做对了再奖励吃的，以此类推就完成了强化训练的过程\n",
    "\n",
    "Agent（操作员，算法）----action（动作）----environment（环境）----observation，reward（观察结果/回报）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化训练环境\n",
    "    1.卸载你的keras和tensorflow\n",
    "    2.重新安装keras和tensorflow\n",
    "        pip install tensorflow==1.14\n",
    "        pip install keras==2.2\n",
    "    3.安装gym环境\n",
    "        pip install gym\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game over\n",
      "Episode:1 Score:42.0\n",
      "game over\n",
      "Episode:2 Score:28.0\n",
      "game over\n",
      "Episode:3 Score:24.0\n",
      "game over\n",
      "Episode:4 Score:15.0\n",
      "game over\n",
      "Episode:5 Score:25.0\n",
      "game over\n",
      "Episode:6 Score:31.0\n",
      "game over\n",
      "Episode:7 Score:12.0\n",
      "game over\n",
      "Episode:8 Score:13.0\n",
      "game over\n",
      "Episode:9 Score:24.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')  # 建立游戏\n",
    "for episode in range(1,10):    # for循环控制游戏次数\n",
    "    state = env.reset()       # 重置游戏\n",
    "    done = False             #初始化完成状态False，即进行中    \n",
    "    score = 0                 #奖励积分\n",
    "    while not done:           #游戏主循环\n",
    "        env.render()            #渲染游戏界面\n",
    "        action = random.choice([0,1])       #随机游戏动作\n",
    "        observation, reward, done, info = env.step(action) #返回发生数据\n",
    "        score += reward                #添加积分\n",
    "        if done == True:              #判断游戏是否失败\n",
    "            print(\"game over\")\n",
    "    print('Episode:{} Score:{}'.format(episode, score))       #输出当前的积分和轮次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习代码\n",
    "    1.当前环境是tensorflow2.5+,\n",
    "     from rl.agents import DQNAgent 会报错no model named 'rl'\n",
    "     先pip install keras-rl\n",
    "    2.安装keras-rl2,不是keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent\n",
    "# from rl.policy import EpsGreedyQPolicy\n",
    "# from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24,activation='relu'))\n",
    "    model.add(Dense(24,activation='relu'))\n",
    "    model.add(Dense(actions,activation='linear'))\n",
    "    return model\n",
    "model = build_model(states, actions)\n",
    "# model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建agent玩家 玩家具备三个条件 策略 记忆和玩家本身\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model,memory=memory,policy=policy,\n",
    "                   nb_actions=actions,nb_steps_warmup=10)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  999/10000 [=>............................] - ETA: 7:14 - reward: 1.0000done, took 48.640 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(optimizer='adam',metrics=['mae'])\n",
    "dqn.fit(env=env, nb_steps=1000,visualize=True,verbose=1) #控制训练次数和可视化\n",
    "dqn.save_weights('../output/rl/model_10000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whfo\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 9.000, steps: 9\n",
      "Episode 2: reward: 8.000, steps: 8\n",
      "Episode 3: reward: 9.000, steps: 9\n",
      "Episode 4: reward: 9.000, steps: 9\n",
      "Episode 5: reward: 8.000, steps: 8\n",
      "Episode 6: reward: 10.000, steps: 10\n",
      "Episode 7: reward: 10.000, steps: 10\n",
      "Episode 8: reward: 10.000, steps: 10\n",
      "Episode 9: reward: 10.000, steps: 10\n",
      "Episode 10: reward: 10.000, steps: 10\n",
      "Episode 11: reward: 10.000, steps: 10\n",
      "Episode 12: reward: 11.000, steps: 11\n",
      "Episode 13: reward: 10.000, steps: 10\n",
      "Episode 14: reward: 10.000, steps: 10\n",
      "Episode 15: reward: 9.000, steps: 9\n",
      "Episode 16: reward: 9.000, steps: 9\n",
      "Episode 17: reward: 10.000, steps: 10\n",
      "Episode 18: reward: 9.000, steps: 9\n",
      "Episode 19: reward: 10.000, steps: 10\n",
      "Episode 20: reward: 9.000, steps: 9\n",
      "Episode 21: reward: 10.000, steps: 10\n",
      "Episode 22: reward: 9.000, steps: 9\n",
      "Episode 23: reward: 9.000, steps: 9\n",
      "Episode 24: reward: 9.000, steps: 9\n",
      "Episode 25: reward: 9.000, steps: 9\n",
      "Episode 26: reward: 10.000, steps: 10\n",
      "Episode 27: reward: 9.000, steps: 9\n",
      "Episode 28: reward: 10.000, steps: 10\n",
      "Episode 29: reward: 9.000, steps: 9\n",
      "Episode 30: reward: 9.000, steps: 9\n",
      "Episode 31: reward: 10.000, steps: 10\n",
      "Episode 32: reward: 11.000, steps: 11\n",
      "Episode 33: reward: 9.000, steps: 9\n",
      "Episode 34: reward: 8.000, steps: 8\n",
      "Episode 35: reward: 10.000, steps: 10\n",
      "Episode 36: reward: 9.000, steps: 9\n",
      "Episode 37: reward: 9.000, steps: 9\n",
      "Episode 38: reward: 10.000, steps: 10\n",
      "Episode 39: reward: 9.000, steps: 9\n",
      "Episode 40: reward: 10.000, steps: 10\n",
      "Episode 41: reward: 10.000, steps: 10\n",
      "Episode 42: reward: 8.000, steps: 8\n",
      "Episode 43: reward: 9.000, steps: 9\n",
      "Episode 44: reward: 10.000, steps: 10\n",
      "Episode 45: reward: 10.000, steps: 10\n",
      "Episode 46: reward: 10.000, steps: 10\n",
      "Episode 47: reward: 8.000, steps: 8\n",
      "Episode 48: reward: 9.000, steps: 9\n",
      "Episode 49: reward: 9.000, steps: 9\n",
      "Episode 50: reward: 9.000, steps: 9\n",
      "Episode 51: reward: 9.000, steps: 9\n",
      "Episode 52: reward: 10.000, steps: 10\n",
      "Episode 53: reward: 8.000, steps: 8\n",
      "Episode 54: reward: 9.000, steps: 9\n",
      "Episode 55: reward: 10.000, steps: 10\n",
      "Episode 56: reward: 10.000, steps: 10\n",
      "Episode 57: reward: 10.000, steps: 10\n",
      "Episode 58: reward: 9.000, steps: 9\n",
      "Episode 59: reward: 9.000, steps: 9\n",
      "Episode 60: reward: 9.000, steps: 9\n",
      "Episode 61: reward: 10.000, steps: 10\n",
      "Episode 62: reward: 8.000, steps: 8\n",
      "Episode 63: reward: 10.000, steps: 10\n",
      "Episode 64: reward: 9.000, steps: 9\n",
      "Episode 65: reward: 9.000, steps: 9\n",
      "Episode 66: reward: 9.000, steps: 9\n",
      "Episode 67: reward: 9.000, steps: 9\n",
      "Episode 68: reward: 9.000, steps: 9\n",
      "Episode 69: reward: 10.000, steps: 10\n",
      "Episode 70: reward: 9.000, steps: 9\n",
      "Episode 71: reward: 9.000, steps: 9\n",
      "Episode 72: reward: 10.000, steps: 10\n",
      "Episode 73: reward: 9.000, steps: 9\n",
      "Episode 74: reward: 9.000, steps: 9\n",
      "Episode 75: reward: 10.000, steps: 10\n",
      "Episode 76: reward: 10.000, steps: 10\n",
      "Episode 77: reward: 9.000, steps: 9\n",
      "Episode 78: reward: 10.000, steps: 10\n",
      "Episode 79: reward: 9.000, steps: 9\n",
      "Episode 80: reward: 10.000, steps: 10\n",
      "Episode 81: reward: 8.000, steps: 8\n",
      "Episode 82: reward: 8.000, steps: 8\n",
      "Episode 83: reward: 9.000, steps: 9\n",
      "Episode 84: reward: 9.000, steps: 9\n",
      "Episode 85: reward: 8.000, steps: 8\n",
      "Episode 86: reward: 9.000, steps: 9\n",
      "Episode 87: reward: 10.000, steps: 10\n",
      "Episode 88: reward: 9.000, steps: 9\n",
      "Episode 89: reward: 9.000, steps: 9\n",
      "Episode 90: reward: 10.000, steps: 10\n",
      "Episode 91: reward: 9.000, steps: 9\n",
      "Episode 92: reward: 9.000, steps: 9\n",
      "Episode 93: reward: 8.000, steps: 8\n",
      "Episode 94: reward: 8.000, steps: 8\n",
      "Episode 95: reward: 10.000, steps: 10\n",
      "Episode 96: reward: 10.000, steps: 10\n",
      "Episode 97: reward: 11.000, steps: 11\n",
      "Episode 98: reward: 10.000, steps: 10\n",
      "Episode 99: reward: 9.000, steps: 9\n",
      "Episode 100: reward: 9.000, steps: 9\n"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(optimizer='adam',metrics=['mae'])\n",
    "dqn.load_weights('../output/rl/model_10000.h5')\n",
    "whf15 = dqn.test(env,nb_episodes=100,visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stark overflow中找到的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whfo\\.conda\\envs\\py38\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whfo\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 3:22:54 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whfo\\.conda\\envs\\py38\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 375s 37ms/step - reward: 1.00001s -  - ETA: 0s - reward: 1.\n",
      "105 episodes - episode_reward: 93.771 [9.000, 200.000] - loss: 2.474 - mae: 19.024 - mean_q: 38.572\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      " 1496/10000 [===>..........................] - ETA: 4:34 - reward: 1.0000"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(1,states)))\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24,activation='relu'))\n",
    "    model.add(Dense(24,activation='relu'))\n",
    "    model.add(Dense(actions,activation='linear'))\n",
    "    return model\n",
    "\n",
    "def build_agent(model,actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions =actions,\n",
    "                   nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "model = build_model(states, actions)\n",
    "model.summary()\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize= False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
